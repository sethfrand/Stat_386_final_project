[
  {
    "objectID": "TechnicalReport.html",
    "href": "TechnicalReport.html",
    "title": "Technical Report",
    "section": "",
    "text": "This project analyzes Utah residential housing listings in several cities in Salt Lake County and Utah counties to explore, build, and evaluate predictive models for house price.\nThe motivation for this project is to provide data-driven guidance for buyers, sellers, and local planners by identifying key predictors of sale price and producing models that balance predictive accuracy and interpretability.\nSome key goals of this project were (1) to produce clear exploratory analyses that reveal relationships between price and common housing attributes; (2) to fit defensible regression models (i.e. best-subsets by AIC and a LASSO-regularized model) and compare them using cross-validated PMSE; and (3) to document reproducible code and a small interactive Streamlit app for sharing results."
  },
  {
    "objectID": "TechnicalReport.html#introduction",
    "href": "TechnicalReport.html#introduction",
    "title": "Technical Report",
    "section": "",
    "text": "This project analyzes Utah residential housing listings in several cities in Salt Lake County and Utah counties to explore, build, and evaluate predictive models for house price.\nThe motivation for this project is to provide data-driven guidance for buyers, sellers, and local planners by identifying key predictors of sale price and producing models that balance predictive accuracy and interpretability.\nSome key goals of this project were (1) to produce clear exploratory analyses that reveal relationships between price and common housing attributes; (2) to fit defensible regression models (i.e. best-subsets by AIC and a LASSO-regularized model) and compare them using cross-validated PMSE; and (3) to document reproducible code and a small interactive Streamlit app for sharing results."
  },
  {
    "objectID": "TechnicalReport.html#data-source-and-methodology",
    "href": "TechnicalReport.html#data-source-and-methodology",
    "title": "Technical Report",
    "section": "Data Source and Methodology",
    "text": "Data Source and Methodology\n\nData acquisition:\n\n\nRaw listings were collected from UtahRealEstate.com with the repository’s scraping scripts. The primary ingestion function used in this report is data_no_scape() which returns the raw DataFrame used for downstream processing.\n\n\nCleaning pipeline:\n\n\nThe cleaning pipeline performs the following steps: removing obvious duplicates, droping invalid or malformed entries, and standardizing numeric columns (e.g. beds, baths, sqft). For purposes of analysis, we dropped the original address and mls fields.\n\n\nAnalysis workflow:\n\n\nExploratory data analysis involved visual checks (scatterplots and boxplots) to inspect relationships between price and candidate predictors and to assess outliers and heterogeneity across selected cities.\nModel selection involved exhaustive best-subsets regression evaluated by AIC to identify an interpretable model, and LASSO (with cross-validated penalty) to perform automated variable selection and shrinkage. Models were fit with statsmodels and model summaries were constructed with patsy.\nModel assessment: 5-fold cross-validation is used to estimate the predictive mean squared error (PMSE) for competing models.\n\n\nTooling and reproducibility:\n\n\nPrimary libraries: pandas, numpy, matplotlib, seaborn, statsmodels, patsy, scikit-learn.\nProject environment and packaging: see pyproject.toml"
  },
  {
    "objectID": "TechnicalReport.html#eda",
    "href": "TechnicalReport.html#eda",
    "title": "Technical Report",
    "section": "EDA",
    "text": "EDA\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a 2x2 figure with 4 subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('EDA: Price vs Selected Variables', fontsize=16, fontweight='bold')\n\n# Plot 1 (top-left): Scatterplot - price vs bedrooms (or sqft, year_built, etc.)\naxes[0, 0].scatter(df['beds'], df['price'], alpha=0.5, s=50, color='steelblue')\naxes[0, 0].set_xlabel('Bedrooms', fontsize=11)\naxes[0, 0].set_ylabel('Price ($)', fontsize=11)\naxes[0, 0].set_title('Price vs Bedrooms', fontweight='bold')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].set_xlim(0, 10)\naxes[0, 0].set_ylim(0, 3_000_000)\n\n# Plot 2 (top-right): Scatterplot - price vs bathrooms (or sqft, lot_size, etc.)\naxes[0, 1].scatter(df['baths'], df['price'], alpha=0.5, s=50, color='coral')\naxes[0, 1].set_xlabel('Bathrooms', fontsize=11)\naxes[0, 1].set_ylabel('Price ($)', fontsize=11)\naxes[0, 1].set_title('Price vs Bathrooms', fontweight='bold')\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].set_xlim(0, 10)\naxes[0, 1].set_ylim(0, 3_000_000)\n\n# Plot 3 (bottom-left): Boxplot - price by garage (categorical)\naxes[1, 0].scatter(df['sqft'], df['price'], alpha=0.5, s=50)\naxes[1, 0].set_xlabel('Square Feet', fontsize=11)\naxes[1, 0].set_ylabel('Price ($)', fontsize=11)\naxes[1, 0].set_title('Price vs Square Footage', fontweight='bold')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].set_xlim(0, 15_000)\naxes[1, 0].set_ylim(0, 4_000_000)\n\n# Plot 4 (bottom-right): Boxplot - price by city (categorical)\nsub_df = df[df['city'].isin(['lindon', 'orem', 'provo', 'spanish-fork'])]\nsub_df.boxplot(column='price', by='city', ax=axes[1, 1])\naxes[1, 1].set_xlabel('City', fontsize=11)\naxes[1, 1].set_ylabel('Price ($)', fontsize=11)\naxes[1, 1].set_title('Price by City', fontweight='bold')\naxes[1, 1].get_figure().suptitle('')  # remove automatic title from boxplot\naxes[1, 1].set_ylim(0, 2_500_000)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "TechnicalReport.html#link-to-streamlit-app",
    "href": "TechnicalReport.html#link-to-streamlit-app",
    "title": "Technical Report",
    "section": "Link to Streamlit App",
    "text": "Link to Streamlit App\n_TODO: Add a link to Streamlit with a short description"
  },
  {
    "objectID": "TechnicalReport.html#analysis",
    "href": "TechnicalReport.html#analysis",
    "title": "Technical Report",
    "section": "Analysis",
    "text": "Analysis\n\n# Import necessary modules\nimport itertools\nimport statsmodels.formula.api as smf\nfrom patsy import dmatrices\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression, LassoCV, Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\n\n# -----------------------------\n# Treat ONLY city and zipcode as categorical\n# -----------------------------\ncat_vars = ['city', 'zipcode']\nfor c in cat_vars:\n    if c in df.columns:\n        df[c] = df[c].astype('category')\n\n# Candidate predictors\ncandidates = [c for c in df.columns if c != 'price']\n\n# -----------------------------\n# Best subsets selection via AIC\n# -----------------------------\nbest_aic = np.inf\nbest_formula = None\nbest_model = None\n\nfor k in range(1, len(candidates) + 1):\n    for subset in itertools.combinations(candidates, k):\n        terms = [f\"C({v})\" if v in cat_vars else v for v in subset]\n        formula = \"price ~ \" + \" + \".join(terms)\n        try:\n            model = smf.ols(formula, data=df).fit()\n            if model.aic &lt; best_aic:\n                best_aic = model.aic\n                best_formula = formula\n                best_model = model\n        except Exception:\n            continue\n\nprint(\"\\n==============================\")\nprint(\"BEST SUBSETS (AIC) MODEL\")\nprint(\"==============================\")\nprint(\"Best AIC:\", best_aic)\nprint(\"Best formula:\", best_formula)\nprint(best_model.summary())\n\n# -----------------------------\n# CV PMSE for Best-AIC model\n# -----------------------------\ny_aic, X_aic = dmatrices(best_formula, data=df, return_type='dataframe')\ny_aic = np.ravel(y_aic)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\nmses_aic = []\n\nfor tr, te in kf.split(X_aic):\n    lr = LinearRegression(fit_intercept=False)\n    lr.fit(X_aic.iloc[tr], y_aic[tr])\n    preds = lr.predict(X_aic.iloc[te])\n    mses_aic.append(mean_squared_error(y_aic[te], preds))\n\npmse_aic = np.mean(mses_aic)\n\n# -----------------------------\n# LASSO (lambda = 1 SE rule)\n# -----------------------------\n# Build full design matrix with dummies\nfull_formula = \"price ~ \" + \" + \".join(\n    [f\"C({v})\" if v in cat_vars else v for v in candidates]\n)\n\ny_lasso, X_lasso = dmatrices(full_formula, data=df, return_type='dataframe')\ny_lasso = np.ravel(y_lasso)\n\n# LASSO with standardization\nlasso_cv = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"lasso\", LassoCV(cv=5, random_state=1))\n])\n\nlasso_cv.fit(X_lasso, y_lasso)\n\n# Lambda_1se\nmse_path = lasso_cv.named_steps[\"lasso\"].mse_path_.mean(axis=1)\nmse_std = lasso_cv.named_steps[\"lasso\"].mse_path_.std(axis=1)\nidx_min = np.argmin(mse_path)\nmse_1se = mse_path[idx_min] + mse_std[idx_min]\nidx_1se = np.where(mse_path &lt;= mse_1se)[0][-1]\n\nalpha_1se = lasso_cv.named_steps[\"lasso\"].alphas_[idx_1se]\n\n# Fit LASSO at lambda_1se\nlasso_1se = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"lasso\", Lasso(alpha=alpha_1se))\n])\n\nlasso_1se.fit(X_lasso, y_lasso)\n\n# Selected variables\ncoef = lasso_1se.named_steps[\"lasso\"].coef_\nselected = X_lasso.columns[coef != 0]\n\nprint(\"\\n==============================\")\nprint(\"LASSO SELECTED VARIABLES (λ_1se)\")\nprint(\"==============================\")\nprint(list(selected))\n\n# -----------------------------\n# Refit OLS using LASSO-selected variables (FIXED)\n# -----------------------------\n\nselected_cols = X_lasso.columns[coef != 0]\n\n# Recover original variable names\nselected_vars = set()\n\nfor col in selected_cols:\n    if col.startswith(\"C(\"):\n        # categorical: extract variable name inside C(...)\n        var = col.split(\"[\")[0]          # C(city)\n        var = var.replace(\"C(\", \"\").replace(\")\", \"\")\n        selected_vars.add(f\"C({var})\")\n    else:\n        selected_vars.add(col)\n\nselected_vars = sorted(selected_vars)\n\nprint(\"\\nVariables used in LASSO refit:\")\nprint(selected_vars)\n\nlasso_formula = \"price ~ \" + \" + \".join(selected_vars)\nlasso_refit = smf.ols(lasso_formula, data=df).fit()\n\nprint(\"\\n==============================\")\nprint(\"OLS REFIT USING LASSO VARIABLES\")\nprint(\"==============================\")\nprint(lasso_refit.summary())\n\n# -----------------------------\n# CV PMSE for LASSO-selected model\n# -----------------------------\ny_lasso2, X_lasso2 = dmatrices(lasso_formula, data=df, return_type='dataframe')\ny_lasso2 = np.ravel(y_lasso2)\n\nmses_lasso = []\nfor tr, te in kf.split(X_lasso2):\n    lr = LinearRegression(fit_intercept=False)\n    lr.fit(X_lasso2.iloc[tr], y_lasso2[tr])\n    preds = lr.predict(X_lasso2.iloc[te])\n    mses_lasso.append(mean_squared_error(y_lasso2[te], preds))\n\npmse_lasso = np.mean(mses_lasso)\n\n# -----------------------------\n# Final comparison\n# -----------------------------\nprint(\"\\n==============================\")\nprint(\"CROSS-VALIDATED PMSE COMPARISON\")\nprint(\"==============================\")\nprint(f\"Best Subsets (AIC) PMSE : {pmse_aic:.3f}\")\nprint(f\"LASSO (1SE) PMSE        : {pmse_lasso:.3f}\")\n\n\n==============================\nBEST SUBSETS (AIC) MODEL\n==============================\nBest AIC: 30231.181453641035\nBest formula: price ~ beds + baths + sqft + year_built + lot_size\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.577\nModel:                            OLS   Adj. R-squared:                  0.575\nMethod:                 Least Squares   F-statistic:                     265.4\nDate:                Wed, 17 Dec 2025   Prob (F-statistic):          6.79e-179\nTime:                        16:59:46   Log-Likelihood:                -15110.\nNo. Observations:                 978   AIC:                         3.023e+04\nDf Residuals:                     972   BIC:                         3.026e+04\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1.709e+07   2.69e+06      6.345      0.000    1.18e+07    2.24e+07\nbeds       -3.414e+05   3.96e+04     -8.618      0.000   -4.19e+05   -2.64e+05\nbaths       1.452e+05   5.43e+04      2.672      0.008    3.86e+04    2.52e+05\nsqft         645.0850     31.942     20.196      0.000     582.402     707.767\nyear_built -8704.3009   1362.142     -6.390      0.000   -1.14e+04   -6031.223\nlot_size    5.773e+04   1.02e+04      5.671      0.000    3.78e+04    7.77e+04\n==============================================================================\nOmnibus:                     1750.557   Durbin-Watson:                   1.882\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2673338.480\nSkew:                          11.796   Prob(JB):                         0.00\nKurtosis:                     258.043   Cond. No.                     3.11e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.11e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n==============================\nLASSO SELECTED VARIABLES (λ_1se)\n==============================\n['C(city)[T.american-fork]', 'C(city)[T.cottonwood-heights]', 'C(city)[T.draper]', 'C(city)[T.eagle-mountain]', 'C(city)[T.highland]', 'C(city)[T.holladay]', 'C(city)[T.lehi]', 'C(city)[T.lindon]', 'C(city)[T.midvale]', 'C(city)[T.millcreek]', 'C(city)[T.murray]', 'C(city)[T.orem]', 'C(city)[T.provo]', 'C(city)[T.salt-lake-city]', 'C(city)[T.sandy]', 'C(city)[T.saratoga-springs]', 'C(city)[T.south-jordan]', 'C(city)[T.south-salt-lake]', 'C(city)[T.spanish-fork]', 'C(city)[T.west-jordan]', 'beds', 'baths', 'sqft', 'year_built', 'lot_size', 'garage']\n\nVariables used in LASSO refit:\n['C(city)', 'baths', 'beds', 'garage', 'lot_size', 'sqft', 'year_built']\n\n==============================\nOLS REFIT USING LASSO VARIABLES\n==============================\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.593\nModel:                            OLS   Adj. R-squared:                  0.582\nMethod:                 Least Squares   F-statistic:                     53.23\nDate:                Wed, 17 Dec 2025   Prob (F-statistic):          2.01e-165\nTime:                        16:59:46   Log-Likelihood:                -15091.\nNo. Observations:                 978   AIC:                         3.024e+04\nDf Residuals:                     951   BIC:                         3.037e+04\nDf Model:                          26                                         \nCovariance Type:            nonrobust                                         \n=================================================================================================\n                                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------\nIntercept                      8.897e+06    3.4e+06      2.616      0.009    2.22e+06    1.56e+07\nC(city)[T.american-fork]       6.363e+05   2.94e+05      2.168      0.030    6.02e+04    1.21e+06\nC(city)[T.cottonwood-heights]    6.8e+05   2.88e+05      2.360      0.018    1.15e+05    1.25e+06\nC(city)[T.draper]              2.199e+05   2.85e+05      0.773      0.440   -3.39e+05    7.78e+05\nC(city)[T.eagle-mountain]      3.095e+05   2.94e+05      1.052      0.293   -2.68e+05    8.87e+05\nC(city)[T.highland]            3.528e+05   2.86e+05      1.235      0.217   -2.08e+05    9.13e+05\nC(city)[T.holladay]            9.416e+05   2.83e+05      3.331      0.001    3.87e+05     1.5e+06\nC(city)[T.lehi]                4.983e+05   2.96e+05      1.682      0.093    -8.3e+04    1.08e+06\nC(city)[T.lindon]              7.268e+04    3.2e+05      0.227      0.820   -5.56e+05    7.01e+05\nC(city)[T.midvale]             8.057e+05      3e+05      2.683      0.007    2.16e+05     1.4e+06\nC(city)[T.millcreek]           7.996e+05   2.95e+05      2.713      0.007    2.21e+05    1.38e+06\nC(city)[T.murray]              6.676e+05   2.96e+05      2.257      0.024    8.71e+04    1.25e+06\nC(city)[T.orem]                6.939e+05   2.93e+05      2.369      0.018    1.19e+05    1.27e+06\nC(city)[T.provo]               1.093e+06   2.94e+05      3.714      0.000    5.15e+05    1.67e+06\nC(city)[T.salt-lake-city]       7.65e+05   3.04e+05      2.516      0.012    1.68e+05    1.36e+06\nC(city)[T.sandy]               6.291e+05    2.9e+05      2.169      0.030    5.98e+04     1.2e+06\nC(city)[T.saratoga-springs]    1.296e+05   2.96e+05      0.439      0.661    -4.5e+05     7.1e+05\nC(city)[T.south-jordan]        4.107e+05   2.91e+05      1.409      0.159   -1.61e+05    9.83e+05\nC(city)[T.south-salt-lake]     8.755e+05   4.16e+05      2.105      0.036    5.91e+04    1.69e+06\nC(city)[T.spanish-fork]        5.297e+05    2.9e+05      1.825      0.068      -4e+04     1.1e+06\nC(city)[T.west-jordan]         7.617e+05   2.95e+05      2.578      0.010    1.82e+05    1.34e+06\nbaths                           1.22e+05   5.61e+04      2.177      0.030     1.2e+04    2.32e+05\nbeds                          -3.344e+05      4e+04     -8.363      0.000   -4.13e+05   -2.56e+05\ngarage                        -1.696e+04   2.63e+04     -0.644      0.520   -6.86e+04    3.47e+04\nlot_size                       5.208e+04   1.03e+04      5.065      0.000    3.19e+04    7.23e+04\nsqft                            678.9545     33.569     20.226      0.000     613.076     744.833\nyear_built                    -4906.1679   1718.494     -2.855      0.004   -8278.646   -1533.689\n==============================================================================\nOmnibus:                     1708.334   Durbin-Watson:                   1.931\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2336371.338\nSkew:                          11.218   Prob(JB):                         0.00\nKurtosis:                     241.392   Cond. No.                     3.96e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.96e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n==============================\nCROSS-VALIDATED PMSE COMPARISON\n==============================\nBest Subsets (AIC) PMSE : 2125253174429.225\nLASSO (1SE) PMSE        : 2219847110171.425"
  },
  {
    "objectID": "TechnicalReport.html#conclusion",
    "href": "TechnicalReport.html#conclusion",
    "title": "Technical Report",
    "section": "Conclusion",
    "text": "Conclusion\nThe report compares two model-building strategies: exhaustive best-subsets selection using AIC and LASSO (with the 1-SE rule). Fitted model summaries are generated through statsmodels, and the models are evaluated using 5-fold cross-validated PMSE. The best‐subsets AIC procedure selected a parsimonious model containing only five structural predictors—beds, baths, square footage, year built, and lot size—yielding an (\\(R^2\\)) of 0.577 and the lowest AIC (30231.2) among all candidate models. All included predictors are statistically significant at the \\(\\alpha=0.05\\) level, with square footage and lot size positively associated with price, while the negative coefficient on beds likely reflects multicollinearity with square footage. This model emphasizes interpretability and achieves strong explanatory power with relatively few parameters.\nIn contrast, the LASSO model using the one–standard‐error rule selected a much richer specification that included the same core structural variables, added garage size, and incorporated city as a categorical factor, resulting in 26 fitted coefficients in the refit OLS model. This model achieved a slightly higher in‐sample fit (\\(R^2 = 0.593\\)), suggesting that location effects explain additional variation in housing prices beyond physical characteristics. However, many individual city coefficients are not statistically significant, indicating that while location matters collectively, its contribution is diffuse across levels rather than driven by a small number of dominant cities.\nWhen comparing predictive performance via cross‐validation, the best‐subsets AIC model slightly outperformed the LASSO‐selected model, with a lower PMSE (\\(2.13 \\times 10^{12}\\) vs. \\(2.22 \\times 10^{12}\\)). This suggests that the additional complexity introduced by the LASSO model does not translate into improved out‐of‐sample prediction. Overall, the results highlight a classic bias–variance tradeoff: the LASSO model captures more structure through location effects but incurs higher variance, while the simpler AIC‐selected model provides comparable and slightly superior—predictive accuracy with greater interpretability.\nThere are several limitations to consider. The scraped dataset may suffer from selection bias, as online listings do not represent all transactions, and measurement errors could exist in features such as square footage or number of bedrooms. Additionally, some predictors are coarse (e.g. city) and may obscure spatial heterogeneity. Residual heteroscedasticity and nonlinearity are also possible, suggesting that transforming the price variable (e.g., using \\(\\log(\\texttt{price})\\)) or applying robust, heteroscedasticity-consistent standard errors may be appropriate.\nFuture work could involve incorporating additional predictors such as year built, lot size, and proximity to amenities, or exploring spatial models that explicitly account for location dependence. Alternative loss functions like MAE could be evaluated, and tree-based ensemble methods such as random forests or gradient boosting could be applied, with comparisons of their calibrated uncertainty against linear models. Finally, the best-performing model could potentially be packaged and deployed via an endpoint or dashboard, enabling users to query price predictions based on property attributes.\nFrom a practical standpoint, the results suggest that a sizeable portion of the information needed to predict home prices comes from a small set of basic property features, especially square footage, lot size, and overall layout. It was observed tha adding many location indicators increases model complexity without meaningfully improving predictive accuracy. For real-world use, such as quick price estimates, reporting, or decision support, the simpler AIC-selected model is likely the better choice: it is easier to explain, easier to maintain, and performs just as well (or slightly better) on new data. More complex models may still be useful for deeper analysis of neighborhood effects."
  },
  {
    "objectID": "Documentation.html",
    "href": "Documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "This project is a Python package designed to collect and analyze Utah housing data from UtahRealEstate.com. It focuses on properties in Utah County and Salt Lake County, providing structured data as well as tools for cleaning, visualization, and analysis.\nThe package’s web scraper uses Playwright for browser automation and easy integration into other Python projects."
  },
  {
    "objectID": "Documentation.html#overview",
    "href": "Documentation.html#overview",
    "title": "Documentation",
    "section": "",
    "text": "This project is a Python package designed to collect and analyze Utah housing data from UtahRealEstate.com. It focuses on properties in Utah County and Salt Lake County, providing structured data as well as tools for cleaning, visualization, and analysis.\nThe package’s web scraper uses Playwright for browser automation and easy integration into other Python projects."
  },
  {
    "objectID": "Documentation.html#features",
    "href": "Documentation.html#features",
    "title": "Documentation",
    "section": "Features",
    "text": "Features\n\nScrapes housing listings for multiple cities in Utah\nExtracts details such as:\n\nMLS number\nPrice\nAddress\nBeds, Baths, Square Footage\nYear Built, Lot Size, Garage\nListing Agent\n\nOutputs data as:\n\nPandas DataFrame or\nCSV file\n\nConfigurable:\n\nNumber of listings per city\nTarget cities"
  },
  {
    "objectID": "Documentation.html#installation",
    "href": "Documentation.html#installation",
    "title": "Documentation",
    "section": "Installation",
    "text": "Installation\n\nInstall Playwright browsers (required for scraping):\n#| eval: false\n\npip install playwright\nplaywright install\nThis will download the necessary browser binaries (Chromium, Firefox, WebKit) for Playwright."
  },
  {
    "objectID": "Documentation.html#usage",
    "href": "Documentation.html#usage",
    "title": "Documentation",
    "section": "Usage",
    "text": "Usage\nThe main functionality is exposed via the get_data function in utah_housing_stat386.core, which scrapes data directly. Warning: This function is extremely memory instensive. If static data is sufficient, it is easiest—highly recommneded—to simply use the data_no_scape function instead.\n\nExample: Basic Data Fetching\n\n# Import dependencies\nfrom utah_housing_stat386.core import get_data\nfrom utah_housing_stat386.cleaning import data_no_scape\nimport pandas as pd\nimport nest_asyncio\nnest_asyncio.apply()\n\n\n#####  Dynamic scraping  #####\n\n# Fetch data for specific cities, 5 listings per city, return as DataFrame\ndf = get_data(max_listings=5, cities=['provo', 'salt-lake-city'], output=\"pandas\")\nprint(df.head())\n\n# Save data to CSV file instead instead\nget_data(max_listings=5, output=\"csv\")\n\n\n#####  Static data (RECOMMENDED)  #####\ndf_static = data_no_scape()"
  },
  {
    "objectID": "Documentation.html#configuration",
    "href": "Documentation.html#configuration",
    "title": "Documentation",
    "section": "Configuration",
    "text": "Configuration\n\nmax_listings: Number of listings per city (default: 5)\ncities: List of cities (default: all supported cities)\noutput: \"pandas\" DataFrame or \"csv\" file (default: \"pandas\")\n\nSupported cities include:\n\nUtah County: alpine, american-fork, eagle-mountain, highland, lindon, lehi, orem, provo, saratoga-springs, spanish-fork\nSalt Lake County: draper, holladay, midvale, millcreek, cottonwood-heights, murray, salt-lake-city, sandy, south-jordan, south-salt-lake, sugarhouse, west-jordan, west-valley"
  },
  {
    "objectID": "Documentation.html#data-cleaning",
    "href": "Documentation.html#data-cleaning",
    "title": "Documentation",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe package includes comprehensive data cleaning functions to transform raw scraped data into analysis-ready format.\n\nQuick Start with Cleaned Data\n\nfrom utah_housing_stat386 import get_cleaned_data, cleaned_static_data\n\n# Get cleaned data directly (via scraping, memory-intensive)\ndf_clean = get_cleaned_data(max_listings=10, output=\"pandas\")\nprint(df_clean.head())\n\n# Get static data (highly recommended)\ndf_static_clean = cleaned_static_data()\nprint(df_static_clean.head())\n\n\n\nManual Cleaning Workflow\n\nfrom utah_housing_stat386.cleaning import data_no_scape\nfrom utah_housing_stat386 import get_data, clean_housing_data, remove_duplicates, remove_invalid_entries\n\n# Get raw data (statically)\ndf_raw = data_no_scape()\n\n# Apply cleaning step-by-step\ndf_clean = clean_housing_data(df_raw)\ndf_clean = remove_duplicates(df_clean)\ndf_clean = remove_invalid_entries(df_clean)\n\n\n\nIndividual Cleaning Functions\n\nfrom utah_housing_stat386 import clean_price, clean_lot_size, clean_garage\n\n# Clean specific fields\ndf['price'] = df['price'].apply(clean_price)\ndf['lot_size'] = df['lot_size'].apply(clean_lot_size)  # Converts to acres\ndf['garage'] = df['garage'].apply(clean_garage)  # Extracts garage spaces\n\n\n\nCleaning Functions Reference\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample Input\nExample Output\n\n\n\n\nclean_price()\nConverts price strings to numeric\n“$481,999”\n481999.0\n\n\nclean_numeric_field()\nCleans beds, baths, sqft\n“1,252”\n1252.0\n\n\nclean_year_built()\nValidates year built\n“1919”\n1919\n\n\nclean_lot_size()\nConverts to acres\n“0.10 Ac”\n0.1\n\n\nclean_garage()\nExtracts garage spaces\n“2 Car”\n2\n\n\nclean_housing_data()\nApplies all cleaning\nDataFrame\nCleaned DataFrame\n\n\nremove_duplicates()\nRemoves duplicate listings\nDataFrame\nDeduplicated DataFrame\n\n\nremove_invalid_entries()\nRemoves rows with missing critical data\nDataFrame\nFiltered DataFrame\n\n\ncheck_is_nan()\nChecks whether a value is NaN or empty\nNone / “”\nTrue / False\n\n\nclean_address()\nStandardizes and trims address strings\n“123 Main St,,”\n“123 Main St”\n\n\nclean_city()\nNormalizes city names to lowercase and trims whitespace\n” Provo ”\n“provo”\n\n\nget_cleaned_data()\nFetches data (via get_data), applies cleaning and returns DataFrame or writes CSV\nget_cleaned_data(max_listings=5)\nCleaned DataFrame or path to CSV\n\n\ndata_no_scape()\nLoads the bundled static CSV files and concatenates them into a DataFrame\nn/a\nDataFrame\n\n\ncleaned_static_data()\nLoads static CSVs and returns a cleaned DataFrame (applies cleaning pipeline)\nn/a\nCleaned DataFrame"
  },
  {
    "objectID": "Documentation.html#demo-testing",
    "href": "Documentation.html#demo-testing",
    "title": "Documentation",
    "section": "Demo & Testing",
    "text": "Demo & Testing\nThe package includes demo functionality to get started quickly:\n\nfrom utah_housing_stat386 import run_demo, demo_cleaning, load_demo_data\n\n# Run full demo with sample data\nrun_demo()\n\n# Load demo dataset\ndf_demo = load_demo_data()\n\n# See demo cleaning in action\ndemo_cleaning()\n\nTests are located in the tests/ directory and can be run with:\npytest tests/"
  },
  {
    "objectID": "Documentation.html#license",
    "href": "Documentation.html#license",
    "title": "Documentation",
    "section": "License",
    "text": "License\nMIT 2025"
  },
  {
    "objectID": "Tutorial.html",
    "href": "Tutorial.html",
    "title": "Accessing the Data",
    "section": "",
    "text": "This tutorial demonstrates how to use the utah_housing_stat386 package to scrape, clean, and analyze Utah housing data from UtahRealEstate.com. The package provides tools to collect data from cities in Utah County and Salt Lake County."
  },
  {
    "objectID": "Tutorial.html#installation",
    "href": "Tutorial.html#installation",
    "title": "Accessing the Data",
    "section": "Installation",
    "text": "Installation\nThe first things that you will need to do is install the utah_housing_stat386 package.\npip install utah-housing-stat386\nAfter installing, you’ll also need to install playwright browers to run the scraping functions."
  },
  {
    "objectID": "Tutorial.html#reading-in-the-data",
    "href": "Tutorial.html#reading-in-the-data",
    "title": "Accessing the Data",
    "section": "Reading in the data",
    "text": "Reading in the data\nUsing the package’s scraping functions get_data and get_cleaned_data are extremely memory intensive. Unless, dynamically updated data is required, it is highly recommended to use the functions data_no_scrape and cleaned_static_data, which are used to read in data that has been previously scraped (a process that took several hours). Details for using get_data and get_cleaned_data may be found on the Documentation page.\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom utah_housing_stat386.cleaning import data_no_scape, cleaned_static_data\n\n# Read in the raw, previously scraped data\ndf_raw = data_no_scape()\ndf_raw.head(10)\n\n\n\n\n\n\n\n\nmls\nprice\naddress\nbeds\nbaths\nsqft\nyear_built\nlot_size\ngarage\nagent\ncity\n\n\n\n\n0\n2124118\n$3,495,973\n769 W Ranch Cir, Alpine, UT 84004\n8\n9\n14472\n2008\n1.02 Ac\n2124118.0\nContact Agent Ned Chidester 801-420-7653 Co-Ag...\nalpine\n\n\n1\n2124041\n$3,850,000\n19 E Elk Ct, Alpine, UT 84004\n7\n7\n8036\n2020\n0.61 Ac\n2124041.0\nContact Agent Tayte Lackey 208-427-4460 Your N...\nalpine\n\n\n2\n2122882\n$719,000\n475 Grove Dr, Alpine, UT 84004\n5\n2\n2150\n1969\n0.50 Ac\n2122882.0\nContact Agent Caleb Eastman 801-419-4773 Your ...\nalpine\n\n\n3\n2121304\n$1,500,000\n48 N 100 E, Alpine, UT 84004\n4\n4\n3808\n1900\n0.93 Ac\n2121304.0\nContact Agent Will S Jones 801-376-7239 Your N...\nalpine\n\n\n4\n2119151\n$1,300,000\n177 W Canyon Crest Rd, Alpine, UT 84004\n5\n3\n3818\n2002\n0.47 Ac\n2119151.0\nContact Agent Cristie Berg 801-372-2751 Co-Age...\nalpine\n\n\n5\n2118246\n$2,245,000\n259 S Twin River Loop, Alpine, UT 84004\n7\n6\n9119\n2003\n0.56 Ac\n2.0\nContact Agent Kerry Oman 801-369-2507 Co-Agent...\nalpine\n\n\n6\n2116152\n$1,350,000\n1063 E Alpine Dr, Alpine, UT 84004\n6\n4\n5102\n1983\n0.46 Ac\n2116152.0\nContact Agent Kara Ragsdale 801-870-6073 Your ...\nalpine\n\n\n7\n2116014\n$1,499,000\n667 E 770 N, Alpine, UT 84004\n9\n5\n6262\n2022\n0.55 Ac\n2.0\nContact Agent Curry Jones 801-830-8199 Your Na...\nalpine\n\n\n8\n2115239\n$1,235,000\n19 N Lone Peak Dr, Alpine, UT 84004\n5\n5\n5587\n1993\n0.90 Ac\n1.0\nContact Agent Julie B. Pierce 801-830-9292 Co-...\nalpine\n\n\n9\n2115034\n$2,400,000\n1801 N Fort Canyon Rd, Alpine, UT 84004\n6\n3\n4491\n1980\n1.74 Ac\n2115034.0\nContact Agent Brett R Sellick 801-502-9955 You...\nalpine\n\n\n\n\n\n\n\n\n# Read in the clean, previously scraped data\ndf_clean = cleaned_static_data()\ndf_clean.head(10)\n\n\n\n\n\n\n\n\nmls\nprice\naddress\nbeds\nbaths\nsqft\nyear_built\nlot_size\ngarage\ncity\n\n\n\n\n0\n2124118\n3495973.0\n769 W Ranch Cir, Alpine, UT 84004\n8.0\n9.0\n14472.0\n2008\n1.02\n0\nalpine\n\n\n1\n2124041\n3850000.0\n19 E Elk Ct, Alpine, UT 84004\n7.0\n7.0\n8036.0\n2020\n0.61\n0\nalpine\n\n\n2\n2122882\n719000.0\n475 Grove Dr, Alpine, UT 84004\n5.0\n2.0\n2150.0\n1969\n0.50\n0\nalpine\n\n\n3\n2121304\n1500000.0\n48 N 100 E, Alpine, UT 84004\n4.0\n4.0\n3808.0\n1900\n0.93\n0\nalpine\n\n\n4\n2119151\n1300000.0\n177 W Canyon Crest Rd, Alpine, UT 84004\n5.0\n3.0\n3818.0\n2002\n0.47\n0\nalpine\n\n\n5\n2118246\n2245000.0\n259 S Twin River Loop, Alpine, UT 84004\n7.0\n6.0\n9119.0\n2003\n0.56\n2\nalpine\n\n\n6\n2116152\n1350000.0\n1063 E Alpine Dr, Alpine, UT 84004\n6.0\n4.0\n5102.0\n1983\n0.46\n0\nalpine\n\n\n7\n2116014\n1499000.0\n667 E 770 N, Alpine, UT 84004\n9.0\n5.0\n6262.0\n2022\n0.55\n2\nalpine\n\n\n8\n2115239\n1235000.0\n19 N Lone Peak Dr, Alpine, UT 84004\n5.0\n5.0\n5587.0\n1993\n0.90\n1\nalpine\n\n\n9\n2115034\n2400000.0\n1801 N Fort Canyon Rd, Alpine, UT 84004\n6.0\n3.0\n4491.0\n1980\n1.74\n0\nalpine"
  },
  {
    "objectID": "Tutorial.html#cleaning-the-data",
    "href": "Tutorial.html#cleaning-the-data",
    "title": "Accessing the Data",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nIf the data was scraped, the following cleaning functions may be used to clean it. For time and simplicity, the following example usage of these functions will be used with raw, previously scraped (i.e. static) data, df_raw.\nCleaning the data step-by-step, function-by-function:\n\nfrom utah_housing_stat386.cleaning import clean_price, clean_numeric_field, clean_year_built, clean_lot_size, clean_garage, clean_address, clean_city\n\n# Apply cleaning step-by-step\ndf_clean1 = df_raw.copy()\ndf_clean1.drop(columns=['agent'])\ndf_clean1['price'] = df_clean1['price'].apply(clean_price)\ndf_clean1['beds'] = df_clean1['beds'].apply(clean_numeric_field)\ndf_clean1['baths'] = df_clean1['baths'].apply(clean_numeric_field)\ndf_clean1['sqft'] = df_clean1['sqft'].apply(clean_numeric_field)\ndf_clean1['year_built'] = df_clean1['year_built'].apply(clean_year_built)\ndf_clean1['lot_size'] = df_clean1['lot_size'].apply(clean_lot_size)\ndf_clean1['garage'] = df_clean1['garage'].apply(clean_garage)\ndf_clean1['address'] = df_clean1['address'].apply(clean_address)\ndf_clean1['city'] = df_clean1['city'].apply(clean_city)\n\ndf_clean1.head(10)\n\n\n\n\n\n\n\n\nmls\nprice\naddress\nbeds\nbaths\nsqft\nyear_built\nlot_size\ngarage\nagent\ncity\n\n\n\n\n0\n2124118\n3495973.0\n769 W Ranch Cir, Alpine, UT 84004\n8.0\n9.0\n14472.0\n2008\n1.02\n0\nContact Agent Ned Chidester 801-420-7653 Co-Ag...\nalpine\n\n\n1\n2124041\n3850000.0\n19 E Elk Ct, Alpine, UT 84004\n7.0\n7.0\n8036.0\n2020\n0.61\n0\nContact Agent Tayte Lackey 208-427-4460 Your N...\nalpine\n\n\n2\n2122882\n719000.0\n475 Grove Dr, Alpine, UT 84004\n5.0\n2.0\n2150.0\n1969\n0.50\n0\nContact Agent Caleb Eastman 801-419-4773 Your ...\nalpine\n\n\n3\n2121304\n1500000.0\n48 N 100 E, Alpine, UT 84004\n4.0\n4.0\n3808.0\n1900\n0.93\n0\nContact Agent Will S Jones 801-376-7239 Your N...\nalpine\n\n\n4\n2119151\n1300000.0\n177 W Canyon Crest Rd, Alpine, UT 84004\n5.0\n3.0\n3818.0\n2002\n0.47\n0\nContact Agent Cristie Berg 801-372-2751 Co-Age...\nalpine\n\n\n5\n2118246\n2245000.0\n259 S Twin River Loop, Alpine, UT 84004\n7.0\n6.0\n9119.0\n2003\n0.56\n2\nContact Agent Kerry Oman 801-369-2507 Co-Agent...\nalpine\n\n\n6\n2116152\n1350000.0\n1063 E Alpine Dr, Alpine, UT 84004\n6.0\n4.0\n5102.0\n1983\n0.46\n0\nContact Agent Kara Ragsdale 801-870-6073 Your ...\nalpine\n\n\n7\n2116014\n1499000.0\n667 E 770 N, Alpine, UT 84004\n9.0\n5.0\n6262.0\n2022\n0.55\n2\nContact Agent Curry Jones 801-830-8199 Your Na...\nalpine\n\n\n8\n2115239\n1235000.0\n19 N Lone Peak Dr, Alpine, UT 84004\n5.0\n5.0\n5587.0\n1993\n0.90\n1\nContact Agent Julie B. Pierce 801-830-9292 Co-...\nalpine\n\n\n9\n2115034\n2400000.0\n1801 N Fort Canyon Rd, Alpine, UT 84004\n6.0\n3.0\n4491.0\n1980\n1.74\n0\nContact Agent Brett R Sellick 801-502-9955 You...\nalpine\n\n\n\n\n\n\n\nCleaning the data with clean_housing_data, which applies all cleaning functions to the DataFrame:\n\nfrom utah_housing_stat386.cleaning import clean_housing_data, remove_duplicates, remove_invalid_entries\n\n# Apply cleaning in consolidated steps\ndf_clean2 = clean_housing_data(df_raw)\ndf_clean2 = remove_duplicates(df_clean2)\ndf_clean2 = remove_invalid_entries(df_clean2)\n\ndf_clean2.head(10)\n\n\n\n\n\n\n\n\nmls\nprice\naddress\nbeds\nbaths\nsqft\nyear_built\nlot_size\ngarage\ncity\n\n\n\n\n0\n2124118\n3495973.0\n769 W Ranch Cir, Alpine, UT 84004\n8.0\n9.0\n14472.0\n2008\n1.02\n0\nalpine\n\n\n1\n2124041\n3850000.0\n19 E Elk Ct, Alpine, UT 84004\n7.0\n7.0\n8036.0\n2020\n0.61\n0\nalpine\n\n\n2\n2122882\n719000.0\n475 Grove Dr, Alpine, UT 84004\n5.0\n2.0\n2150.0\n1969\n0.50\n0\nalpine\n\n\n3\n2121304\n1500000.0\n48 N 100 E, Alpine, UT 84004\n4.0\n4.0\n3808.0\n1900\n0.93\n0\nalpine\n\n\n4\n2119151\n1300000.0\n177 W Canyon Crest Rd, Alpine, UT 84004\n5.0\n3.0\n3818.0\n2002\n0.47\n0\nalpine\n\n\n5\n2118246\n2245000.0\n259 S Twin River Loop, Alpine, UT 84004\n7.0\n6.0\n9119.0\n2003\n0.56\n2\nalpine\n\n\n6\n2116152\n1350000.0\n1063 E Alpine Dr, Alpine, UT 84004\n6.0\n4.0\n5102.0\n1983\n0.46\n0\nalpine\n\n\n7\n2116014\n1499000.0\n667 E 770 N, Alpine, UT 84004\n9.0\n5.0\n6262.0\n2022\n0.55\n2\nalpine\n\n\n8\n2115239\n1235000.0\n19 N Lone Peak Dr, Alpine, UT 84004\n5.0\n5.0\n5587.0\n1993\n0.90\n1\nalpine\n\n\n9\n2115034\n2400000.0\n1801 N Fort Canyon Rd, Alpine, UT 84004\n6.0\n3.0\n4491.0\n1980\n1.74\n0\nalpine\n\n\n\n\n\n\n\nPerform some last cleaning steps:\n\nimport re\n\ndf = df_clean2.copy()\n\n# Extract zipcodes\naddress_list = df['address'].tolist()\npattern = r\"UT \\d{5}\"\nzipcode_list = []\nfor address in address_list:\n    zipcode_list.append(re.search(pattern, address).group()[3:])\ndf['zipcode'] = zipcode_list\ndf = df.drop(columns=['address', 'mls'])\n\ndf\n\n\n\n\n\n\n\n\nprice\nbeds\nbaths\nsqft\nyear_built\nlot_size\ngarage\ncity\nzipcode\n\n\n\n\n0\n3495973.0\n8.0\n9.0\n14472.0\n2008\n1.02\n0\nalpine\n84004\n\n\n1\n3850000.0\n7.0\n7.0\n8036.0\n2020\n0.61\n0\nalpine\n84004\n\n\n2\n719000.0\n5.0\n2.0\n2150.0\n1969\n0.50\n0\nalpine\n84004\n\n\n3\n1500000.0\n4.0\n4.0\n3808.0\n1900\n0.93\n0\nalpine\n84004\n\n\n4\n1300000.0\n5.0\n3.0\n3818.0\n2002\n0.47\n0\nalpine\n84004\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n509\n520000.0\n4.0\n3.0\n2005.0\n1995\n0.16\n0\nwest-jordan\n84088\n\n\n510\n598990.0\n3.0\n3.0\n2433.0\n2025\n0.08\n0\nwest-jordan\n84081\n\n\n511\n1054810.0\n6.0\n4.0\n4903.0\n2025\n0.14\n2\nwest-jordan\n84081\n\n\n512\n589900.0\n3.0\n3.0\n2861.0\n2022\n0.11\n0\nwest-jordan\n84081\n\n\n513\n1049990.0\n5.0\n4.0\n5020.0\n2025\n0.18\n0\nsouth-jordan\n84009\n\n\n\n\n979 rows × 9 columns\n\n\n\nThe data is now ready for EDA and analysis (see the Technical Report page)."
  },
  {
    "objectID": "tests/test.html",
    "href": "tests/test.html",
    "title": "Utah Housing Data Package - STAT 386",
    "section": "",
    "text": "from utah_housing_stat386 import get_data\n\n\nimport nest_asyncio\nnest_asyncio.apply()\n\nimport pandas as pd\n\ndf = get_data(max_listings=5, cities = ['provo'], output=\"pandas\")\nprint(df.head())\n\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬──────┐\n\n│ requests_finished             │ 0    │\n\n│ requests_failed               │ 0    │\n\n│ retry_histogram               │ [0]  │\n\n│ request_avg_failed_duration   │ None │\n\n│ request_avg_finished_duration │ None │\n\n│ requests_finished_per_minute  │ 0    │\n\n│ requests_failed_per_minute    │ 0    │\n\n│ request_total_duration        │ 0s   │\n\n│ requests_total                │ 0    │\n\n│ crawler_runtime               │ 0s   │\n\n└───────────────────────────────┴──────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/24 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 3; cpu = 0.0; mem = 0.0; event_loop = 0.382; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/24 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/provo-homes due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x10c9f5c10&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/24 pages, 0 failed requests, desired concurrency 4.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/24 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/29 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/29 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.94 GB of 2.00 GB (97%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────┐\n\n│ requests_finished             │ 1      │\n\n│ requests_failed               │ 0      │\n\n│ retry_histogram               │ [0, 1] │\n\n│ request_avg_failed_duration   │ None   │\n\n│ request_avg_finished_duration │ 15.65s │\n\n│ requests_finished_per_minute  │ 1      │\n\n│ requests_failed_per_minute    │ 0      │\n\n│ request_total_duration        │ 15.65s │\n\n│ requests_total                │ 1      │\n\n│ crawler_runtime               │ 60.00s │\n\n└───────────────────────────────┴────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/29 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 5; desired_concurrency = 5; cpu = 0.0; mem = 0.291; event_loop = 0.024; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.08 GB of 2.00 GB (104%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/29 pages, 0 failed requests, desired concurrency 4.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  Waiting for remaining tasks to finish\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Error analysis: total_errors=1 unique_errors=1\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Final request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 6          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [5, 1]     │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 37.07s     │\n\n│ requests_finished_per_minute  │ 5          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 3min 42.4s │\n\n│ requests_total                │ 6          │\n\n│ crawler_runtime               │ 1min 18.6s │\n\n└───────────────────────────────┴────────────┘\n\n\n\n\n       mls     price                         address beds baths  sqft  \\\n0  2123494  $481,999    615 E 420 N, Provo, UT 84606    4     2  1252   \n1  2124209  $300,000    751 S 400 W, Provo, UT 84601    4     3  1922   \n2  2124187  $469,000   1051 E 300 N, Provo, UT 84606    3     2  1272   \n3  2123875  $350,000    126 S 700 W, Provo, UT 84601    5     3  2636   \n4  2123462  $553,300  1587 N 3190 W, Provo, UT 84601    3     3  2068   \n\n  year_built lot_size   garage  \\\n0       1919  0.10 Ac        0   \n1       1894  0.15 Ac            \n2       1961  0.22 Ac  2124187   \n3       1919  0.16 Ac        4   \n4       2025  0.07 Ac  2123462   \n\n                                               agent   city  \n0  Contact Agent Samuel Brinton 801-583-2020 Your...  provo  \n1  Contact Agent Tawna Marsh 801-636-5268 Your Na...  provo  \n2  Contact Agent Tiffany Fletcher 801-800-6365 Yo...  provo  \n3  Contact Agent Cody Thorup 385-695-8382 Your Na...  provo  \n4  Contact Agent C Terry Clark 801-550-0903 Your ...  provo  \n\n\n\ndf\n\n\n\n\n\n\n\n\nmls\nprice\naddress\nbeds\nbaths\nsqft\nyear_built\nlot_size\ngarage\nagent\ncity\n\n\n\n\n0\n2123494\n$481,999\n615 E 420 N, Provo, UT 84606\n4\n2\n1252\n1919\n0.10 Ac\n0\nContact Agent Samuel Brinton 801-583-2020 Your...\nprovo\n\n\n1\n2124209\n$300,000\n751 S 400 W, Provo, UT 84601\n4\n3\n1922\n1894\n0.15 Ac\n\nContact Agent Tawna Marsh 801-636-5268 Your Na...\nprovo\n\n\n2\n2124187\n$469,000\n1051 E 300 N, Provo, UT 84606\n3\n2\n1272\n1961\n0.22 Ac\n2124187\nContact Agent Tiffany Fletcher 801-800-6365 Yo...\nprovo\n\n\n3\n2123875\n$350,000\n126 S 700 W, Provo, UT 84601\n5\n3\n2636\n1919\n0.16 Ac\n4\nContact Agent Cody Thorup 385-695-8382 Your Na...\nprovo\n\n\n4\n2123462\n$553,300\n1587 N 3190 W, Provo, UT 84601\n3\n3\n2068\n2025\n0.07 Ac\n2123462\nContact Agent C Terry Clark 801-550-0903 Your ...\nprovo\n\n\n\n\n\n\n\n\nfrom utah_housing_stat386.core import get_data\nimport pandas as pd\nimport nest_asyncio\nnest_asyncio.apply()\n\ndf = get_data(max_listings=5, cities = ['orem'], output=\"pandas\")\ndf\n\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬──────┐\n\n│ requests_finished             │ 0    │\n\n│ requests_failed               │ 0    │\n\n│ retry_histogram               │ [0]  │\n\n│ request_avg_failed_duration   │ None │\n\n│ request_avg_finished_duration │ None │\n\n│ requests_finished_per_minute  │ 0    │\n\n│ requests_failed_per_minute    │ 0    │\n\n│ request_total_duration        │ 0s   │\n\n│ requests_total                │ 0    │\n\n│ crawler_runtime               │ 0s   │\n\n└───────────────────────────────┴──────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/2 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 3; cpu = 0.0; mem = 0.0; event_loop = 0.446; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/2 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/2 pages, 0 failed requests, desired concurrency 4.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.92 GB of 2.00 GB (96%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────┐\n\n│ requests_finished             │ 1      │\n\n│ requests_failed               │ 0      │\n\n│ retry_histogram               │ [1]    │\n\n│ request_avg_failed_duration   │ None   │\n\n│ request_avg_finished_duration │ 23.44s │\n\n│ requests_finished_per_minute  │ 1      │\n\n│ requests_failed_per_minute    │ 0      │\n\n│ request_total_duration        │ 23.44s │\n\n│ requests_total                │ 1      │\n\n│ crawler_runtime               │ 59.76s │\n\n└───────────────────────────────┴────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2127119 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2127037 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126977 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126815 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126591 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 5; cpu = 0.0; mem = 0.195; event_loop = 0.038; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2127119 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2127037 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126977 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126815 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126591 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.95 GB of 2.00 GB (97%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 4.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.48 GB of 2.00 GB (124%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.07 GB of 2.00 GB (104%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 2.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.21 GB of 2.00 GB (111%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 1          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1]        │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 23.44s     │\n\n│ requests_finished_per_minute  │ 1          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 23.44s     │\n\n│ requests_total                │ 1          │\n\n│ crawler_runtime               │ 1min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.23 GB of 2.00 GB (111%). Consider increasing available memory.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 1; cpu = 0.038; mem = 1.0; event_loop = 0.019; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.17 GB of 2.00 GB (109%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.05 GB of 2.00 GB (103%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.05 GB of 2.00 GB (102%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 1          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1]        │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 23.44s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 23.44s     │\n\n│ requests_total                │ 1          │\n\n│ crawler_runtime               │ 2min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 1; cpu = 0.0; mem = 1.0; event_loop = 0.0; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.03 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.02 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (100%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.97 GB of 2.00 GB (99%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.94 GB of 2.00 GB (97%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 1          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1]        │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 23.44s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 23.44s     │\n\n│ requests_total                │ 1          │\n\n│ crawler_runtime               │ 3min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 1; cpu = 0.0; mem = 1.0; event_loop = 0.0; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.94 GB of 2.00 GB (97%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 1          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1]        │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 23.44s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 23.44s     │\n\n│ requests_total                │ 1          │\n\n│ crawler_runtime               │ 4min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.27 GB of 2.00 GB (113%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 1; cpu = 0.0; mem = 0.765; event_loop = 0.043; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.00 GB of 2.00 GB (100%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.02 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.02 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.02 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.91 GB of 2.00 GB (96%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 2          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1, 0, 1]  │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 20.45s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 40.91s     │\n\n│ requests_total                │ 2          │\n\n│ crawler_runtime               │ 5min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 1; desired_concurrency = 1; cpu = 0.0; mem = 0.732; event_loop = 0.0; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (100%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.11 GB of 2.00 GB (106%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.12 GB of 2.00 GB (106%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.13 GB of 2.00 GB (107%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.03 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 3          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1, 0, 2]  │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 18.83s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 56.49s     │\n\n│ requests_total                │ 3          │\n\n│ crawler_runtime               │ 6min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 1; desired_concurrency = 1; cpu = 0.0; mem = 0.77; event_loop = 0.0; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.26 GB of 2.00 GB (113%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.03 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.04 GB of 2.00 GB (102%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.04 GB of 2.00 GB (102%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.04 GB of 2.00 GB (102%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.98 GB of 2.00 GB (99%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 4          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1, 0, 3]  │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 18.04s     │\n\n│ requests_finished_per_minute  │ 1          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 1min 12.2s │\n\n│ requests_total                │ 4          │\n\n│ crawler_runtime               │ 7min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 1; desired_concurrency = 1; cpu = 0.0; mem = 0.654; event_loop = 0.0; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.24 GB of 2.00 GB (112%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 5/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 5/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.96 GB of 2.00 GB (98%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 5/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.21 GB of 2.00 GB (110%). Consider increasing available memory.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  Waiting for remaining tasks to finish\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Error analysis: total_errors=10 unique_errors=2\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Final request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 6          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1, 0, 5]  │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 20.04s     │\n\n│ requests_finished_per_minute  │ 1          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 2min 0.2s  │\n\n│ requests_total                │ 6          │\n\n│ crawler_runtime               │ 8min 48.2s │\n\n└───────────────────────────────┴────────────┘\n\n\n\n\n\n\n\n\n\n\n\nmls\nprice\naddress\nbeds\nbaths\nsqft\nyear_built\nlot_size\ngarage\nagent\ncity\n\n\n\n\n0\n2127119\n$625,000\n560 S 500 E, Orem, UT 84097\n5\n2\n3194\n1957\n0.24 Ac\n560\nContact Agent Rodney B Moser 801-930-0945 Co-A...\norem\n\n\n1\n2127037\n$479,900\n847 W 630 N, Orem, UT 84057\n5\n2\n1728\n1970\n0.17 Ac\n2127037\nContact Agent Eric Tolman 801-830-3142 Co-Agen...\norem\n\n\n2\n2126977\n$274,900\n1140 W 950 N #402, Orem, UT 84057\n2\n1\n845\n2021\n0.02 Ac\n0\nContact Agent Devin Haub 801-857-7214 Your Nam...\norem\n\n\n3\n2126815\n$539,900\n380 E 1000 N, Orem, UT 84057\n4\n2\n1728\n1970\n0.22 Ac\n2126815\nContact Agent Martin Sommerkamp-Mendizabal 801...\norem\n\n\n4\n2126591\n$920,000\n756 S 900 E, Orem, UT 84097\n6\n4\n4866\n2015\n0.44 Ac\n2\nContact Agent Ashley Axley 801-592-1199 Your N...\norem"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Utah Housing Data Package",
    "section": "",
    "text": "Wasatch Mountains\n\n\nThis project involves the collection and analysis of Utah housing data from UtahRealEstate.com. It focuses on properties in Utah County and Salt Lake County. The project also involved the creation of Python package utah-housing-stat386, which provides structured data as well as tools for cleaning, visualization, and analysis.\nDocumentation for the package may be found here.\nA simple tutorial using the package’s functions may be found here.\nThe technical report summarizing the project may be found here."
  }
]