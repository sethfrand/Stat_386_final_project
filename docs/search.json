[
  {
    "objectID": "TechnicalReport.html",
    "href": "TechnicalReport.html",
    "title": "Technical Report",
    "section": "",
    "text": "TODO: Provide a concise overview of the project objectives, key findings, and actionable recommendations ."
  },
  {
    "objectID": "TechnicalReport.html#executive-summary",
    "href": "TechnicalReport.html#executive-summary",
    "title": "Technical Report",
    "section": "",
    "text": "TODO: Provide a concise overview of the project objectives, key findings, and actionable recommendations ."
  },
  {
    "objectID": "TechnicalReport.html#project-context",
    "href": "TechnicalReport.html#project-context",
    "title": "Technical Report",
    "section": "Project Context",
    "text": "Project Context\nTODO: Describe the motivation, stakeholders, and success criteria for this analysis."
  },
  {
    "objectID": "TechnicalReport.html#data-sources",
    "href": "TechnicalReport.html#data-sources",
    "title": "Technical Report",
    "section": "Data Sources",
    "text": "Data Sources\n\nPrimary dataset: Salt_Lake_City_Water_Quality_Data.csv and utah_housing_data.csv\nSupplementary data: TODO - describe any external references\nData access notes: To retrieve this data is quite time consuming. Best to do the demo."
  },
  {
    "objectID": "TechnicalReport.html#methodology",
    "href": "TechnicalReport.html#methodology",
    "title": "Technical Report",
    "section": "Methodology",
    "text": "Methodology\n\nData acquisition: TODO - outline collection scripts/APIs\nCleaning pipeline: TODO - summarize transformations and validations implemented\nAnalysis workflow: TODO - describe statistical or modeling techniques\nTooling: TODO - list packages, environments, and reproducibility steps"
  },
  {
    "objectID": "TechnicalReport.html#results-diagnostics",
    "href": "TechnicalReport.html#results-diagnostics",
    "title": "Technical Report",
    "section": "Results & Diagnostics",
    "text": "Results & Diagnostics\nTODO: Summarize the main metrics, charts, or model diagnostics produced. Include links to figures or tables once available."
  },
  {
    "objectID": "TechnicalReport.html#discussion-next-steps",
    "href": "TechnicalReport.html#discussion-next-steps",
    "title": "Technical Report",
    "section": "Discussion & Next Steps",
    "text": "Discussion & Next Steps\nTODO: Interpret the results, note limitations, and capture open questions or future experiments."
  },
  {
    "objectID": "Documentation.html",
    "href": "Documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "This project is a Python package designed to collect and analyze Utah housing data from UtahRealEstate.com. It focuses on properties in Utah County and Salt Lake County, providing structured data as well as tools for cleaning, visualization, and analysis.\nThe package’s web scraper uses Playwright for browser automation and easy integration into other Python projects."
  },
  {
    "objectID": "Documentation.html#overview",
    "href": "Documentation.html#overview",
    "title": "Documentation",
    "section": "",
    "text": "This project is a Python package designed to collect and analyze Utah housing data from UtahRealEstate.com. It focuses on properties in Utah County and Salt Lake County, providing structured data as well as tools for cleaning, visualization, and analysis.\nThe package’s web scraper uses Playwright for browser automation and easy integration into other Python projects."
  },
  {
    "objectID": "Documentation.html#features",
    "href": "Documentation.html#features",
    "title": "Documentation",
    "section": "Features",
    "text": "Features\n\nScrapes housing listings for multiple cities in Utah\nExtracts details such as:\n\nMLS number\nPrice\nAddress\nBeds, Baths, Square Footage\nYear Built, Lot Size, Garage\nListing Agent\n\nOutputs data as:\n\nPandas DataFrame or\nCSV file\n\nConfigurable:\n\nNumber of listings per city\nTarget cities"
  },
  {
    "objectID": "Documentation.html#project-structure",
    "href": "Documentation.html#project-structure",
    "title": "Documentation",
    "section": "Project Structure",
    "text": "Project Structure\nStat_386_final_project/\n├── LICENSE\n├── README.md\n├── Documentation.qmd\n├── Tutorial.qmd\n├── TechnicalReport.qmd\n├── index.qmd\n├── pyproject.toml\n├── streamlit_page.py\n├── styles.css\n├── uv.lock\n├── data/\n│   ├── Salt_Lake_County_housing_data.csv\n│   ├── test_data.csv\n│   └── utah_housing_data_ORIGINAL.csv\n├── scripts/\n│   ├── _scraper_less_intensive.py\n│   ├── salt_lake_county.py\n│   └── scraper.py\n├── src/\n│   └── utah_housing_stat386/\n│       ├── __init__.py\n│       ├── core.py\n│       ├── cleaning.py\n│       ├── demo.py\n│       └── data/\n├── tests/\n│   ├── package_test.py\n│   ├── test_cleaning.py\n│   └── test.ipynb\n└── docs/\n    └── (generated Quarto HTML files)"
  },
  {
    "objectID": "Documentation.html#package",
    "href": "Documentation.html#package",
    "title": "Documentation",
    "section": "Package",
    "text": "Package\nThe main package is utah_housing_stat386, located in the src/utah_housing_stat386/ directory. It contains the core functionality for scraping, cleaning, and data handling.\n\ncore.py: Contains the main scraping logic and data fetching functions\ncleaning.py: Data cleaning and validation functions for housing data\ndemo.py: Demo functions for testing and quick prototyping\n__init__.py: Initializes the package and exposes all public functions"
  },
  {
    "objectID": "Documentation.html#installation",
    "href": "Documentation.html#installation",
    "title": "Documentation",
    "section": "Installation",
    "text": "Installation\n\nInstall Playwright browsers (required for scraping):\n#| eval: false\n\npip install playwright\nplaywright install\nThis will download the necessary browser binaries (Chromium, Firefox, WebKit) for Playwright."
  },
  {
    "objectID": "Documentation.html#usage",
    "href": "Documentation.html#usage",
    "title": "Documentation",
    "section": "Usage",
    "text": "Usage\nThe main functionality is exposed via the get_data function in utah_housing_stat386.core, which scrapes data directly. Warning: This function is extremely memory instensive. If static data is sufficient, it is easiest—highly recommneded—to simply use the data_no_scape function instead.\n\nExample: Basic Data Fetching\n\n# Import dependencies\nfrom utah_housing_stat386.core import get_data\nfrom utah_housing_stat386.cleaning import data_no_scape\nimport pandas as pd\nimport nest_asyncio\nnest_asyncio.apply()\n\n\n#####  Dynamic scraping  #####\n\n# Fetch data for specific cities, 5 listings per city, return as DataFrame\ndf = get_data(max_listings=5, cities=['provo', 'salt-lake-city'], output=\"pandas\")\nprint(df.head())\n\n# Save data to CSV file instead instead\nget_data(max_listings=5, output=\"csv\")\n\n\n#####  Static data (RECOMMENDED)  #####\ndf_static = data_no_scape()"
  },
  {
    "objectID": "Documentation.html#configuration",
    "href": "Documentation.html#configuration",
    "title": "Documentation",
    "section": "Configuration",
    "text": "Configuration\n\nmax_listings: Number of listings per city (default: 5)\ncities: List of cities (default: all supported cities)\noutput: \"pandas\" DataFrame or \"csv\" file (default: \"pandas\")\n\nSupported cities include:\n\nUtah County: alpine, american-fork, eagle-mountain, highland, lindon, lehi, orem, provo, saratoga-springs, spanish-fork\nSalt Lake County: draper, holladay, midvale, millcreek, cottonwood-heights, murray, salt-lake-city, sandy, south-jordan, south-salt-lake, sugarhouse, west-jordan, west-valley"
  },
  {
    "objectID": "Documentation.html#data-files",
    "href": "Documentation.html#data-files",
    "title": "Documentation",
    "section": "Data Files",
    "text": "Data Files\n\ndata/utah_housing_data_ORIGINAL.csv: Sample of scraped data for Utah County\ndata/Salt_Lake_County_housing_data.csv: Sample of scraped data for Salt Lake County\ndata/test_data.csv: Test dataset (produced in development)"
  },
  {
    "objectID": "Documentation.html#scripts-produced-in-development",
    "href": "Documentation.html#scripts-produced-in-development",
    "title": "Documentation",
    "section": "Scripts (produced in development)",
    "text": "Scripts (produced in development)\n\nscripts/scraper.py: Main scraper script using Playwright\nscripts/_scraper_less_intensive.py: Less intensive version of the scraper\nscripts/salt_lake_county.py: Script to scrape Salt Lake County data"
  },
  {
    "objectID": "Documentation.html#other-files-resources",
    "href": "Documentation.html#other-files-resources",
    "title": "Documentation",
    "section": "Other Files & Resources",
    "text": "Other Files & Resources\n\npyproject.toml: Project configuration, dependencies, and package metadata\nuv.lock: Lock file for reproducible dependency management\nstreamlit_page.py: Interactive Streamlit web interface for data exploration\nTutorial.qmd: User guide and tutorial for the package\nTechnicalReport.qmd: Detailed technical documentation and methodology\ntests/: Unit tests and integration tests for the package\ndocs/: Pre-built Quarto HTML documentation files"
  },
  {
    "objectID": "Documentation.html#data-cleaning",
    "href": "Documentation.html#data-cleaning",
    "title": "Documentation",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe package includes comprehensive data cleaning functions to transform raw scraped data into analysis-ready format.\n\nQuick Start with Cleaned Data\n\nfrom utah_housing_stat386 import get_cleaned_data, cleaned_static_data\n\n# Get cleaned data directly (via scraping, memory-intensive)\ndf_clean = get_cleaned_data(max_listings=10, output=\"pandas\")\nprint(df_clean.head())\n\n# Get static data (highly recommended)\ndf_static_clean = cleaned_static_data()\nprint(df_static_clean.head())\n\n\n\nManual Cleaning Workflow\n\nfrom utah_housing_stat386.cleaning import data_no_scape\nfrom utah_housing_stat386 import get_data, clean_housing_data, remove_duplicates, remove_invalid_entries\n\n# Get raw data (statically)\ndf_raw = data_no_scape()\n\n# Apply cleaning step-by-step\ndf_clean = clean_housing_data(df_raw)\ndf_clean = remove_duplicates(df_clean)\ndf_clean = remove_invalid_entries(df_clean)\n\n\n\nIndividual Cleaning Functions\n\nfrom utah_housing_stat386 import clean_price, clean_lot_size, clean_garage\n\n# Clean specific fields\ndf['price'] = df['price'].apply(clean_price)\ndf['lot_size'] = df['lot_size'].apply(clean_lot_size)  # Converts to acres\ndf['garage'] = df['garage'].apply(clean_garage)  # Extracts garage spaces\n\n\n\nCleaning Functions Reference\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample Input\nExample Output\n\n\n\n\nclean_price()\nConverts price strings to numeric\n“$481,999”\n481999.0\n\n\nclean_numeric_field()\nCleans beds, baths, sqft\n“1,252”\n1252.0\n\n\nclean_year_built()\nValidates year built\n“1919”\n1919\n\n\nclean_lot_size()\nConverts to acres\n“0.10 Ac”\n0.1\n\n\nclean_garage()\nExtracts garage spaces\n“2 Car”\n2\n\n\nclean_housing_data()\nApplies all cleaning\nDataFrame\nCleaned DataFrame\n\n\nremove_duplicates()\nRemoves duplicate listings\nDataFrame\nDeduplicated DataFrame\n\n\nremove_invalid_entries()\nRemoves rows with missing critical data\nDataFrame\nFiltered DataFrame\n\n\ncheck_is_nan()\nChecks whether a value is NaN or empty\nNone / “”\nTrue / False\n\n\nclean_address()\nStandardizes and trims address strings\n“123 Main St,,”\n“123 Main St”\n\n\nclean_city()\nNormalizes city names to lowercase and trims whitespace\n” Provo ”\n“provo”\n\n\nget_cleaned_data()\nFetches data (via get_data), applies cleaning and returns DataFrame or writes CSV\nget_cleaned_data(max_listings=5)\nCleaned DataFrame or path to CSV\n\n\ndata_no_scape()\nLoads the bundled static CSV files and concatenates them into a DataFrame\nn/a\nDataFrame\n\n\ncleaned_static_data()\nLoads static CSVs and returns a cleaned DataFrame (applies cleaning pipeline)\nn/a\nCleaned DataFrame"
  },
  {
    "objectID": "Documentation.html#demo-testing",
    "href": "Documentation.html#demo-testing",
    "title": "Documentation",
    "section": "Demo & Testing",
    "text": "Demo & Testing\nThe package includes demo functionality to get started quickly:\n\nfrom utah_housing_stat386 import run_demo, demo_cleaning, load_demo_data\n\n# Run full demo with sample data\nrun_demo()\n\n# Load demo dataset\ndf_demo = load_demo_data()\n\n# See demo cleaning in action\ndemo_cleaning()\n\nTests are located in the tests/ directory and can be run with:\n#| eval: false\n\npytest tests/"
  },
  {
    "objectID": "Documentation.html#license",
    "href": "Documentation.html#license",
    "title": "Documentation",
    "section": "License",
    "text": "License\nMIT 2025"
  },
  {
    "objectID": "Tutorial.html",
    "href": "Tutorial.html",
    "title": "Getting Started",
    "section": "",
    "text": "This tutorial demonstrates how to use the utah_housing_stat386 package to scrape, clean, and analyze Utah housing data from UtahRealEstate.com. The package provides tools to collect data from cities in Utah County and Salt Lake County."
  },
  {
    "objectID": "Tutorial.html#installation",
    "href": "Tutorial.html#installation",
    "title": "Getting Started",
    "section": "Installation",
    "text": "Installation\nThe first things that you will need to do is install the utah_housing_stat386 package. pip install utah-housing-stat386\nimport utah_housing as\nAfter installing, you’ll also need to install playwright browers to run the scraping functions.\nThis package includes a demo datset that you can use so that you can start exploring the data immediately!\nfrom utah_housing_stat386 import demo_data\n\n#load the demo dataset\n\ndf = load_demo_data()\n\n#Display the basic information of the dataset\n\nprint(f\"Dataset Shape: {df.shape}\")\nprint(f\"\\nDataset Columns: {df.columns}\")\nprint(f\"\\nDataset Description: \\n{df.describe()}\")\nprint(f\"\\nFirst few rows: \\n{df.head()}\")"
  },
  {
    "objectID": "Tutorial.html#understanding-the-data",
    "href": "Tutorial.html#understanding-the-data",
    "title": "Getting Started",
    "section": "Understanding the Data",
    "text": "Understanding the Data\nTo understand the datatypes that we can, you can run\nprint(\"Data Types:\"\nprint(df.dtypes())\n\nprint(\"\\nSummary Statistics:\")\ndf.describe()"
  },
  {
    "objectID": "tests/test.html",
    "href": "tests/test.html",
    "title": "Utah Housing Data Package - STAT 386",
    "section": "",
    "text": "from utah_housing_stat386 import get_data\n\n\nimport nest_asyncio\nnest_asyncio.apply()\n\nimport pandas as pd\n\ndf = get_data(max_listings=5, cities = ['provo'], output=\"pandas\")\nprint(df.head())\n\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬──────┐\n\n│ requests_finished             │ 0    │\n\n│ requests_failed               │ 0    │\n\n│ retry_histogram               │ [0]  │\n\n│ request_avg_failed_duration   │ None │\n\n│ request_avg_finished_duration │ None │\n\n│ requests_finished_per_minute  │ 0    │\n\n│ requests_failed_per_minute    │ 0    │\n\n│ request_total_duration        │ 0s   │\n\n│ requests_total                │ 0    │\n\n│ crawler_runtime               │ 0s   │\n\n└───────────────────────────────┴──────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/24 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 3; cpu = 0.0; mem = 0.0; event_loop = 0.382; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/24 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/provo-homes due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x10c9f5c10&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/24 pages, 0 failed requests, desired concurrency 4.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/24 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/29 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/29 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.94 GB of 2.00 GB (97%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────┐\n\n│ requests_finished             │ 1      │\n\n│ requests_failed               │ 0      │\n\n│ retry_histogram               │ [0, 1] │\n\n│ request_avg_failed_duration   │ None   │\n\n│ request_avg_finished_duration │ 15.65s │\n\n│ requests_finished_per_minute  │ 1      │\n\n│ requests_failed_per_minute    │ 0      │\n\n│ request_total_duration        │ 15.65s │\n\n│ requests_total                │ 1      │\n\n│ crawler_runtime               │ 60.00s │\n\n└───────────────────────────────┴────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/29 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 5; desired_concurrency = 5; cpu = 0.0; mem = 0.291; event_loop = 0.024; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.08 GB of 2.00 GB (104%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/29 pages, 0 failed requests, desired concurrency 4.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  Waiting for remaining tasks to finish\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Error analysis: total_errors=1 unique_errors=1\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Final request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 6          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [5, 1]     │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 37.07s     │\n\n│ requests_finished_per_minute  │ 5          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 3min 42.4s │\n\n│ requests_total                │ 6          │\n\n│ crawler_runtime               │ 1min 18.6s │\n\n└───────────────────────────────┴────────────┘\n\n\n\n\n       mls     price                         address beds baths  sqft  \\\n0  2123494  $481,999    615 E 420 N, Provo, UT 84606    4     2  1252   \n1  2124209  $300,000    751 S 400 W, Provo, UT 84601    4     3  1922   \n2  2124187  $469,000   1051 E 300 N, Provo, UT 84606    3     2  1272   \n3  2123875  $350,000    126 S 700 W, Provo, UT 84601    5     3  2636   \n4  2123462  $553,300  1587 N 3190 W, Provo, UT 84601    3     3  2068   \n\n  year_built lot_size   garage  \\\n0       1919  0.10 Ac        0   \n1       1894  0.15 Ac            \n2       1961  0.22 Ac  2124187   \n3       1919  0.16 Ac        4   \n4       2025  0.07 Ac  2123462   \n\n                                               agent   city  \n0  Contact Agent Samuel Brinton 801-583-2020 Your...  provo  \n1  Contact Agent Tawna Marsh 801-636-5268 Your Na...  provo  \n2  Contact Agent Tiffany Fletcher 801-800-6365 Yo...  provo  \n3  Contact Agent Cody Thorup 385-695-8382 Your Na...  provo  \n4  Contact Agent C Terry Clark 801-550-0903 Your ...  provo  \n\n\n\ndf\n\n\n\n\n\n\n\n\nmls\nprice\naddress\nbeds\nbaths\nsqft\nyear_built\nlot_size\ngarage\nagent\ncity\n\n\n\n\n0\n2123494\n$481,999\n615 E 420 N, Provo, UT 84606\n4\n2\n1252\n1919\n0.10 Ac\n0\nContact Agent Samuel Brinton 801-583-2020 Your...\nprovo\n\n\n1\n2124209\n$300,000\n751 S 400 W, Provo, UT 84601\n4\n3\n1922\n1894\n0.15 Ac\n\nContact Agent Tawna Marsh 801-636-5268 Your Na...\nprovo\n\n\n2\n2124187\n$469,000\n1051 E 300 N, Provo, UT 84606\n3\n2\n1272\n1961\n0.22 Ac\n2124187\nContact Agent Tiffany Fletcher 801-800-6365 Yo...\nprovo\n\n\n3\n2123875\n$350,000\n126 S 700 W, Provo, UT 84601\n5\n3\n2636\n1919\n0.16 Ac\n4\nContact Agent Cody Thorup 385-695-8382 Your Na...\nprovo\n\n\n4\n2123462\n$553,300\n1587 N 3190 W, Provo, UT 84601\n3\n3\n2068\n2025\n0.07 Ac\n2123462\nContact Agent C Terry Clark 801-550-0903 Your ...\nprovo\n\n\n\n\n\n\n\n\nfrom utah_housing_stat386.core import get_data\nimport pandas as pd\nimport nest_asyncio\nnest_asyncio.apply()\n\ndf = get_data(max_listings=5, cities = ['orem'], output=\"pandas\")\ndf\n\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬──────┐\n\n│ requests_finished             │ 0    │\n\n│ requests_failed               │ 0    │\n\n│ retry_histogram               │ [0]  │\n\n│ request_avg_failed_duration   │ None │\n\n│ request_avg_finished_duration │ None │\n\n│ requests_finished_per_minute  │ 0    │\n\n│ requests_failed_per_minute    │ 0    │\n\n│ request_total_duration        │ 0s   │\n\n│ requests_total                │ 0    │\n\n│ crawler_runtime               │ 0s   │\n\n└───────────────────────────────┴──────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/2 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 3; cpu = 0.0; mem = 0.0; event_loop = 0.446; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/2 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 0/2 pages, 0 failed requests, desired concurrency 4.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.92 GB of 2.00 GB (96%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────┐\n\n│ requests_finished             │ 1      │\n\n│ requests_failed               │ 0      │\n\n│ retry_histogram               │ [1]    │\n\n│ request_avg_failed_duration   │ None   │\n\n│ request_avg_finished_duration │ 23.44s │\n\n│ requests_finished_per_minute  │ 1      │\n\n│ requests_failed_per_minute    │ 0      │\n\n│ request_total_duration        │ 23.44s │\n\n│ requests_total                │ 1      │\n\n│ crawler_runtime               │ 59.76s │\n\n└───────────────────────────────┴────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2127119 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2127037 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126977 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126815 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126591 due to: Page.goto: Timeout 30000ms exceeded.File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result,     raise self._exception.with_traceback(self._exception_tb)\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 5; cpu = 0.0; mem = 0.195; event_loop = 0.038; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2127119 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2127037 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126977 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126815 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee.crawlers._playwright._playwright_crawler] WARN  Retrying request to https://www.utahrealestate.com/listing/2126591 due to: Creating a new page with plugin &lt;crawlee.browsers._playwright_browser_plugin.PlaywrightBrowserPlugin object at 0x11b47a1d0&gt; timed out.,  File \"/Users/cromacair/Desktop/STAT 386/Final Project/Final_Project_Repo/.venv/lib/python3.11/site-packages/playwright/_impl/_connection.py\", line 123, in _inner_send,     done, _ = await asyncio.wait(\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.95 GB of 2.00 GB (97%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 5.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 4.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.48 GB of 2.00 GB (124%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 3.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.07 GB of 2.00 GB (104%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 2.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.21 GB of 2.00 GB (111%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 1          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1]        │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 23.44s     │\n\n│ requests_finished_per_minute  │ 1          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 23.44s     │\n\n│ requests_total                │ 1          │\n\n│ crawler_runtime               │ 1min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.23 GB of 2.00 GB (111%). Consider increasing available memory.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 1; cpu = 0.038; mem = 1.0; event_loop = 0.019; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.17 GB of 2.00 GB (109%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.05 GB of 2.00 GB (103%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.05 GB of 2.00 GB (102%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 1          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1]        │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 23.44s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 23.44s     │\n\n│ requests_total                │ 1          │\n\n│ crawler_runtime               │ 2min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 1; cpu = 0.0; mem = 1.0; event_loop = 0.0; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.03 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.02 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (100%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.97 GB of 2.00 GB (99%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.94 GB of 2.00 GB (97%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 1          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1]        │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 23.44s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 23.44s     │\n\n│ requests_total                │ 1          │\n\n│ crawler_runtime               │ 3min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 1; cpu = 0.0; mem = 1.0; event_loop = 0.0; client_info = 0.0\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.94 GB of 2.00 GB (97%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 1          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1]        │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 23.44s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 23.44s     │\n\n│ requests_total                │ 1          │\n\n│ crawler_runtime               │ 4min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.27 GB of 2.00 GB (113%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 1/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 0; desired_concurrency = 1; cpu = 0.0; mem = 0.765; event_loop = 0.043; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.00 GB of 2.00 GB (100%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.02 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.02 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.02 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.91 GB of 2.00 GB (96%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 2          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1, 0, 1]  │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 20.45s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 40.91s     │\n\n│ requests_total                │ 2          │\n\n│ crawler_runtime               │ 5min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 2/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 1; desired_concurrency = 1; cpu = 0.0; mem = 0.732; event_loop = 0.0; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.01 GB of 2.00 GB (100%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.11 GB of 2.00 GB (106%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.12 GB of 2.00 GB (106%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.13 GB of 2.00 GB (107%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.03 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 3          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1, 0, 2]  │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 18.83s     │\n\n│ requests_finished_per_minute  │ 0          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 56.49s     │\n\n│ requests_total                │ 3          │\n\n│ crawler_runtime               │ 6min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 3/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 1; desired_concurrency = 1; cpu = 0.0; mem = 0.77; event_loop = 0.0; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.26 GB of 2.00 GB (113%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.03 GB of 2.00 GB (101%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.04 GB of 2.00 GB (102%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.04 GB of 2.00 GB (102%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.04 GB of 2.00 GB (102%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.98 GB of 2.00 GB (99%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Current request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 4          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1, 0, 3]  │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 18.04s     │\n\n│ requests_finished_per_minute  │ 1          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 1min 12.2s │\n\n│ requests_total                │ 4          │\n\n│ crawler_runtime               │ 7min 59.8s │\n\n└───────────────────────────────┴────────────┘\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  current_concurrency = 1; desired_concurrency = 1; cpu = 0.0; mem = 0.654; event_loop = 0.0; client_info = 0.0\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.24 GB of 2.00 GB (112%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 4/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 5/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 5/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 1.96 GB of 2.00 GB (98%). Consider increasing available memory.\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Crawled 5/7 pages, 0 failed requests, desired concurrency 1.\n\n[crawlee._autoscaling.snapshotter] WARN  Memory is critically overloaded. Using 2.21 GB of 2.00 GB (110%). Consider increasing available memory.\n\n[crawlee._autoscaling.autoscaled_pool] INFO  Waiting for remaining tasks to finish\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Error analysis: total_errors=10 unique_errors=2\n\n[crawlee.crawlers._playwright._playwright_crawler] INFO  Final request statistics:\n\n┌───────────────────────────────┬────────────┐\n\n│ requests_finished             │ 6          │\n\n│ requests_failed               │ 0          │\n\n│ retry_histogram               │ [1, 0, 5]  │\n\n│ request_avg_failed_duration   │ None       │\n\n│ request_avg_finished_duration │ 20.04s     │\n\n│ requests_finished_per_minute  │ 1          │\n\n│ requests_failed_per_minute    │ 0          │\n\n│ request_total_duration        │ 2min 0.2s  │\n\n│ requests_total                │ 6          │\n\n│ crawler_runtime               │ 8min 48.2s │\n\n└───────────────────────────────┴────────────┘\n\n\n\n\n\n\n\n\n\n\n\nmls\nprice\naddress\nbeds\nbaths\nsqft\nyear_built\nlot_size\ngarage\nagent\ncity\n\n\n\n\n0\n2127119\n$625,000\n560 S 500 E, Orem, UT 84097\n5\n2\n3194\n1957\n0.24 Ac\n560\nContact Agent Rodney B Moser 801-930-0945 Co-A...\norem\n\n\n1\n2127037\n$479,900\n847 W 630 N, Orem, UT 84057\n5\n2\n1728\n1970\n0.17 Ac\n2127037\nContact Agent Eric Tolman 801-830-3142 Co-Agen...\norem\n\n\n2\n2126977\n$274,900\n1140 W 950 N #402, Orem, UT 84057\n2\n1\n845\n2021\n0.02 Ac\n0\nContact Agent Devin Haub 801-857-7214 Your Nam...\norem\n\n\n3\n2126815\n$539,900\n380 E 1000 N, Orem, UT 84057\n4\n2\n1728\n1970\n0.22 Ac\n2126815\nContact Agent Martin Sommerkamp-Mendizabal 801...\norem\n\n\n4\n2126591\n$920,000\n756 S 900 E, Orem, UT 84097\n6\n4\n4866\n2015\n0.44 Ac\n2\nContact Agent Ashley Axley 801-592-1199 Your N...\norem"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Utah Housing Data Package",
    "section": "",
    "text": "Wasatch Mountains\n\n\nThis project involves the collection and analysis of Utah housing data from UtahRealEstate.com. It focuses on properties in Utah County and Salt Lake County. The project also involved the creation of Python package utah-housing-stat386, which provides structured data as well as tools for cleaning, visualization, and analysis.\nDocumentation for the package may be found here.\nA simple tutorial using the package’s functions may be found here.\nThe technical report summarizing the project may be found here."
  }
]